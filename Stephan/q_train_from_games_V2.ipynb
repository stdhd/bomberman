{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_observation_V6'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9318e00e3f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_observation_V6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'create_observation_V6'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import *\n",
    "import matplotlib.pyplot as plt\n",
    "from indices import *\n",
    "from create_observation_V6 import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_train_from_games(reader_path, writer_path):\n",
    "    \"\"\"\n",
    "    Reads all json-files from reader_path\n",
    "    Trains Q table\n",
    "    Writes resulting Q table to writer_path\n",
    "    TODO: iterate over games = state files\n",
    "    \"\"\"\n",
    "    AGENTS_COUNT = 4\n",
    "    OBSERVATION_LENGTH = 8\n",
    "    ACTIONS_COUNT = 6 \n",
    "    \n",
    "    daten = np.load(reader_path)\n",
    "    learned = np.zeros([daten,6])\n",
    "    obs = np.zeros([0, OBSERVATION_LENGTH])\n",
    "    observation_db = np.zeros([daten.shape[0],AGENTS_COUNT,OBSERVATION_LENGTH])\n",
    "    action_db = np.zeros([daten.shape[0],AGENTS_COUNT])\n",
    "    reward_db = np.zeros([daten.shape[0],AGENTS_COUNT])\n",
    "    \n",
    "    \n",
    "    for i in range(daten.shape[0]):\n",
    "        temp_observation, temp_action,temp_reward = create_observation_vectors(daten[i], 2, AGENTS_COUNT)\n",
    "        for j in range(AGENTS_COUNT):\n",
    "            observation_db[i,j], action_db[i,j], reward_db[i,j] = temp_observation[j], temp_action[j],temp_reward[j]\n",
    "                \n",
    "    for k in range(AGENTS_COUNT):\n",
    "            learned, obs = do_action(learned, obs, 0.8, 0.7, observation_db[:,j], action_db[:,j], reward_db[:,j])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_action(learned, obs, learning_rate, discount, observation_db, action_db, reward_db):\n",
    "    last_action_index = -1\n",
    "    last_index = -1\n",
    "    last_reward = -1\n",
    "    for i in range(observation_db.shape[0]):\n",
    "       \n",
    "        current_obs = observation_db[i]\n",
    "        obs, index_current, learned = update_and_get_obs(obs, current_obs, learned)\n",
    "        \n",
    "        choice = action_db[i]\n",
    "        \n",
    "        my_best_value = np.max(learned[index_current])\n",
    "        n_state = get_following_state(state, spielfeld, actions[choice], actions, True)\n",
    "        if (not last_index == -1):\n",
    "            learned[last_index, last_action_index] = (1-learning_rate) * learned[last_index, last_action_index] + learning_rate * (last_reward + discount * my_best_value)\n",
    "            \n",
    "        last_reward = reward_db[i]\n",
    "        last_action_index = choice\n",
    "        last_index = index_current\n",
    "                       \n",
    "    reihe = np.append(reihe, np.array([reihenvalue]))\n",
    "    return learned, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_and_get_obs(db, new_obs, learned):\n",
    "    temp = -1\n",
    "    for i in range(db.shape[0]):\n",
    "        if np.array_equal(db[i], new_obs):\n",
    "            return db, i, learned\n",
    "    db = np.append(db,np.array([new_obs]), axis = 0)\n",
    "    learned = np.append(learned, np.zeros([1,learned.shape[1]]), axis = 0)\n",
    "    return db, (db.shape[0] - 1), learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
