{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taler = 1\n",
    "spielfeld = np.zeros([10,10])\n",
    "spielfeld[0,:] = 2\n",
    "spielfeld[9,:] = 2\n",
    "spielfeld[:,0] = 2\n",
    "spielfeld[:,9] = 2\n",
    "spielfeld[8,8] = 1\n",
    "spielfeld[4,2] = 1\n",
    "spielfeld[6,4] = 1\n",
    "\n",
    "spielfeld = spielfeld.astype(int)\n",
    "state = [1,1]\n",
    "learned = np.zeros([0,4])\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 1, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 1, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spielfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned:\n",
      "[[-183.779982   -380.0001618  -200.           -2.        ]\n",
      " [-218.00577308   -5.62704547 -200.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [  -3.582      -180.           -1.99998      -1.99999998]\n",
      " [   0.            0.           -2.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [-344.39616182 -200.00000178   -2.           -2.        ]\n",
      " [-186.10403452  -36.8728759    -2.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.           -1.998        -1.999998  ]\n",
      " [-361.97996562 -182.00016198   -2.         -200.        ]\n",
      " [-183.97812762 -379.99998      -2.         -200.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [-203.76090056 -218.01063851   -2.           -2.        ]\n",
      " [   0.            0.          198.            0.        ]\n",
      " [-187.40038002   -6.13004584   -2.           -2.        ]\n",
      " [   0.           -7.07651807   -1.8           0.        ]\n",
      " [-185.38212942    0.           -1.99999998   -1.999998  ]\n",
      " [-345.4019838     0.         -199.9998       -2.        ]\n",
      " [-574.20016018 -202.00009337   -2.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.          180.        ]\n",
      " [-563.60038146 -216.28090223   -2.         -200.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [  -7.58700546  -23.67276752   -2.         -200.        ]\n",
      " [-382.32220031 -182.20556965   -2.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [-346.34052715  -23.65454547 -200.           -2.        ]\n",
      " [-400.00142218   -9.42240256 -200.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [-205.06001747 -417.9864597    -2.           -2.        ]\n",
      " [-364.00046766 -224.7586245    -2.         -200.        ]\n",
      " [-346.05039947 -203.9960126  -200.           -2.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "observations:\n",
      "[[3. 2. 2. 0. 0.]\n",
      " [3. 2. 0. 0. 0.]\n",
      " [3. 2. 2. 2. 0.]\n",
      " [3. 2. 2. 0. 2.]\n",
      " [2. 0. 2. 0. 0.]\n",
      " [2. 0. 0. 0. 0.]\n",
      " [2. 2. 2. 2. 0.]\n",
      " [1. 0. 2. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 2. 2. 2. 0.]\n",
      " [2. 0. 0. 0. 1.]\n",
      " [2. 0. 0. 2. 0.]\n",
      " [2. 0. 2. 2. 0.]\n",
      " [1. 0. 2. 2. 2.]\n",
      " [3. 2. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [3. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 0.]\n",
      " [3. 0. 2. 0. 0.]\n",
      " [3. 2. 0. 0. 2.]\n",
      " [3. 0. 0. 0. 2.]\n",
      " [3. 2. 0. 2. 2.]\n",
      " [3. 0. 0. 1. 2.]\n",
      " [0. 0. 0. 2. 2.]\n",
      " [0. 2. 0. 2. 2.]\n",
      " [0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 2.]\n",
      " [0. 0. 2. 2. 2.]\n",
      " [0. 2. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 2.]\n",
      " [0. 2. 2. 0. 2.]\n",
      " [0. 0. 2. 0. 0.]\n",
      " [0. 0. 2. 2. 0.]\n",
      " [0. 2. 2. 0. 0.]\n",
      " [0. 2. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "do_action(state, spielfeld, learned, epsilon)\n",
    "# 1. Richtung des nÃ¤chsten Coins: (1,2,3,4:links,oben,rechts,unten)\n",
    "# 2.-5. Direkte Umgebung: (links, oben, rechts, unten; 0=frei, 1 = Coin, 2 = Wand)\n",
    "# Actions: \"down\", \"up\", \"left\", \"right\"\n",
    "# [3. 2. 2. 0. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_action(state, spielfeld, learned, epsilon):\n",
    "    obs = np.zeros([0,5])\n",
    "    for i in range(100000):\n",
    "        actions = np.array([\"down\", \"up\", \"left\", \"right\"])\n",
    "        discount = 0.2\n",
    "        learning_rate = 0.9\n",
    "        n_state = [0,0]\n",
    "        if False:\n",
    "            print(\"+++++++++++++++++++\")\n",
    "        else:\n",
    "            indices = np.array([])\n",
    "            possible_actions = np.array([])\n",
    "            current_obs = get_observation(spielfeld,state[0],state[1])\n",
    "            # print(current_obs)\n",
    "            obs, index_current, learned = update_and_get_obs(obs, current_obs, learned)\n",
    "            \n",
    "            for j in range(actions.shape[0]):\n",
    "                f_state = get_following_state(state, spielfeld, actions[j], actions, False)\n",
    "                # print(f_state)\n",
    "                new_obs = get_observation(spielfeld,f_state[0],f_state[1])\n",
    "                obs, to_add, learned = update_and_get_obs(obs, new_obs, learned)\n",
    "                indices = np.arange(actions.shape[0])\n",
    "            \n",
    "            possible_actions = actions\n",
    "            indices = indices.astype(int)\n",
    "            values = learned[indices,:]\n",
    "\n",
    "            if epsilon > uniform(0,1):\n",
    "                # epsilon -= 0.001\n",
    "                # print(\"RANDOM\")\n",
    "                b = values.flatten()\n",
    "                optima = np.random.choice(np.flatnonzero(b))\n",
    "                best_choice = possible_actions[optima // actions.shape[0]]\n",
    "                best_value = values.flatten()[optima]\n",
    "            else:\n",
    "                b = values.flatten()\n",
    "                optima = np.random.choice(np.flatnonzero(b == b.max()))\n",
    "                best_choice = possible_actions[optima // actions.shape[0]]\n",
    "                best_value = values.flatten()[optima]\n",
    "\n",
    "            n_state = get_following_state(state, spielfeld, best_choice, actions, True)\n",
    "            action_index = optima // actions.shape[0]\n",
    "                \n",
    "            reward, spielfeld = get_reward(state,spielfeld,best_choice)\n",
    "            learned[index_current, action_index] = (1-learning_rate) * learned[index_current,action_index] + learning_rate * (best_value + discount * reward)\n",
    "            state = n_state\n",
    "    print(\"learned:\")\n",
    "    print(learned)\n",
    "    print(\"observations:\")\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_following_state(state, spielfeld, choice, actions, wall_sensitive):\n",
    "    n_state = np.array([0,0])\n",
    "    if(choice == \"down\"):\n",
    "        n_state[0] = state[0] + 1\n",
    "        n_state[1] = state[1]\n",
    "    elif(choice == \"up\"):\n",
    "        n_state[0] = state[0] - 1\n",
    "        n_state[1] = state[1]\n",
    "    elif(choice == \"left\"):\n",
    "        n_state[0] = state[0]\n",
    "        n_state[1] = state[1] - 1\n",
    "    elif(choice == \"right\"):\n",
    "        n_state[0] = state[0]\n",
    "        n_state[1] = state[1] + 1\n",
    "        \n",
    "    if wall_sensitive and spielfeld[n_state[0], n_state[1]] == 2:\n",
    "        # return old state if action is not possible\n",
    "        return state\n",
    "    \n",
    "    return n_state\n",
    "\n",
    "def get_reward(state,spielfeld,action):\n",
    "    x,y = 0,0\n",
    "    if action == \"left\":\n",
    "        x = -1\n",
    "    if action == \"right\":\n",
    "        x = 1\n",
    "    if action == \"down\":\n",
    "        y = 1\n",
    "    if action == \"up\":\n",
    "        y = -1\n",
    "    \n",
    "    y +=state[0]\n",
    "    x +=state[1] \n",
    "    \n",
    "    if spielfeld[y,x] == 1:\n",
    "        # collect coin\n",
    "        spielfeld[y,x] = 0\n",
    "        return 1000, spielfeld\n",
    "    elif spielfeld[y,x] == 2:\n",
    "        # Mauer\n",
    "        return -1000, spielfeld\n",
    "    else:\n",
    "        return -1, spielfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_observation(spielfeld, y, x):\n",
    "    # Suche den Coin, der am nÃ¤chsten ist (manhattan distance)\n",
    "    coinsY, coinsX = np.where(spielfeld == 1)\n",
    "    distancesY = np.abs(coinsY - y)\n",
    "    distancesX = np.abs(coinsX - x)\n",
    "    observation = np.array([0, 0,0,0,0]) \n",
    "    # 1. Richtung des nÃ¤chsten Coins: (1,2,3,4:links,oben,rechts,unten)\n",
    "    # 2. Direkte Umgebung: (links, oben, rechts, unten; 0=frei, 1 = Coin, 2 = Wand)\n",
    " \n",
    "    if not distancesY.shape[0] == 0:\n",
    "        distances = distancesY + distancesX\n",
    "        coinY = coinsY[np.argmin(distances)]\n",
    "        coinX = coinsX[np.argmin(distances)]\n",
    "        # setze observation flags:\n",
    "        if coinX < x:\n",
    "            observation[0] = 1\n",
    "        elif coinX > x:\n",
    "            observation[0] = 3\n",
    "        elif coinY > y:\n",
    "            observation[0] = 2\n",
    "        elif coinY < y:\n",
    "            observation[0] = 4\n",
    "    \n",
    "    \n",
    "    observation[1] = get_spielfeld(y, x - 1,spielfeld)\n",
    "    observation[2] = get_spielfeld(y - 1, x,spielfeld)\n",
    "    observation[3] = get_spielfeld(y, x + 1,spielfeld)\n",
    "    observation[4] = get_spielfeld(y + 1, x,spielfeld)\n",
    "    # links, oben, rechts, unten\n",
    "    \n",
    "    return observation\n",
    "\n",
    "def update_and_get_obs(db, new_obs, learned):\n",
    "    temp = -1\n",
    "    for i in range(db.shape[0]):\n",
    "        if np.array_equal(db[i], new_obs):\n",
    "            return db, i, learned\n",
    "    db = np.append(db,np.array([new_obs]), axis = 0)\n",
    "    learned = np.append(learned, np.zeros([1,learned.shape[1]]), axis = 0)\n",
    "    return db, (db.shape[0] - 1), learned\n",
    "\n",
    "def get_spielfeld(y,x,spielfeld):\n",
    "    if y > spielfeld.shape[0] - 1 or x > spielfeld.shape[1] - 1 or y < 0 or x < 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return spielfeld[y,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spielfeld(1,0,spielfeld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   3   2]\n",
      " [  4 455   6]\n",
      " [  2   6   8]]\n",
      "[1 6 8]\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "db, y, learned = update_and_get_obs(db, new,learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   3,   2],\n",
       "       [  4, 455,   6],\n",
       "       [  2,   6,   8],\n",
       "       [  1,   6,   8]])"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6., 6.],\n",
       "       [4., 3., 6., 6.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
