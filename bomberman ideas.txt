





Use as policy the simple agent. 



Idea: Learn board state values from observing many games played between simple agents. 




For each game played among agents, create 4 episodes per agent (by rotating the board each way). 



Learn to predict the value of each board state of the agent -> New policy chooses the move which maximizes the expected value of the resultant state. 





First, observe many games between simple agents and use value iteration (?) to find board state values. 



Then, adapt policy to explore and optimize the already learned policy (which is the behavior of simple agents). 




Option 1)

-  Curate features and process only the immediately surrounding game state

Features: 


Direction to move in shortest path to nearest coin

Manhattan distance of nearest coin




Direction to move in shortest path to nearest enemy

Manhattan distance of nearest enemy




Direction to move in shortest path to nearest crate

Manhattan distance of nearest crate



Shortest Manhattan distance of an enemy to the nearest coin


Manhattan distance to nearest enemy

Enemy in explosive line of sight


This player is currently in live bomb radius

(and if yes -> direction of escape) 


Manhattan distance from center of map


Number of remaining enemies


Number of remaining boxes


Total distance to all remaining boxes

Total distance to all remaining enemies


Smallest total distance ---"--- among all enemies



This player is the last remaining player


Option 2)


Save entire board state as a feature vector


Option 3)


Center a restricted (perhaps 13 x 13) board on the agent and set outside areas to walls

(-> do not need to note self position, reduce features by 1 (omit occupied square))


-> Combine with designed features for knowledge about the rest of the board








Approach  -> Collect this information for each step of each game, and train two functions: 



Note: Winning is equivalent to having the highest number of points. Therefore, the move with the highest number of points won until the end of the game is the move with the highest winning probability. 





One function to guess the probability of a win from the current state (alternatively: Total amount of points scored from this point on in the next series of moves/until the end of the round/per move until the end of the round) state value function ("f(s)")



Another function to guess the next state from the current state and decision ("g(s | d)")




Decision function a(s_) = argmax d € D (Expct~g(s | d, s_)[f(g(s | d, s_))])





Idea: Assuming infinite training data, g(s|d, s_) is implicitly given by lim N_s -> +inf sum(1 : s follows s_ given d)/N_s 


It follows => Expct~g(s | d, s_)[f(g(s | d, s_))] = lim N_s -> +inf sum (f_train(s) : s follows s_ given d)/N_s 


(where f_train(s*) refers to the state value target datum for s* in an episode ep_tau according the formula above) 


Problem: Insufficient training data to reasonably model complex board states (?)




Problem: The problem of predicting the next board state from the current board state and decision g(s|d) is too expensive using the methods covered in the lecture. R^(n + 1) -> R^n



Idea: Learn instead f(g(s|d)) =:  k(s | d)  directly --> Regression problem



To do this, first train f(s), then apply f(s) to all training data to create new target labels 




Problem: If we focus only on total number of points earned from a certain step on out, we overvalue early moves and undervalue late moves. 


Idea: Choose the average number of points scored. 



Idea: Generate diverse training data by spawning players in random positions, with random board states, to accurately sample from a variety of states. 


























Problem: 


Using an exploration policy, future board states will result from suboptimal decisions --> This makes for poor training data. 


Example: A game in which a random action is regularly chosen will not accurately represent the flow of a game in which only (locally) best options are chosen. 




Need some way to learn "possible" decisions without directly using the results to estimate board state values or guess new board states.  



New idea: Run a lot of games among normal simple agents, store as normal training data.  


Then: Record random states in these games to use as seeds for new games. 




Now, start new games using these positions, *but* make a random decision (for only one agent) as the first move. Objective: Learn beneficial alternative moves without compromising the value of future board states with noise. Basically: Make a random decision, and finish the game from there using known optimal decisions. 






Current situation: 
----------------


We want to design an agent (function) a: S -> D which, given a board state s € S, chooses among actions ("decisions") D to select the action with the highest expected placement at the end of the game. Each board state is (in the case of Bomberman strictly deterministically) associated with a number of points p(s) € R. Points are collected over the course of the game, and the agent with the highest number of points at the end wins the game. Player placements are in decreasing order of number of points scored. 



Observations: 

#  High placement <=> having many points at the end. Assuming a fixed and not necessarily known policy function a*: S -> D:

#  Move d in board state s leads to more points won until the end of the game than move d' => d results in more points won by the end of the game than d' <=> d results in a greater or equal placement than d' from board state s

#  The optimal move is that move which maximizes the expected number of points won until the end of the game (AFTER the current state) <=> maximizes the expected number of points won per remaining move

#  => Define value function f(s, d) := "Expected number of points won per move until the end of the game from board state s given move d"

#  Per observation 1), argmax (d € D) f(s, d) =: a(s) is the optimal move in board state s. 


Problem: 


How to we learn how many points a board state + move is expected to result in, i.e. how can we learn about f? 

Given (s, d) € S x D , define g(s_ | s, d) as probability mass function over board states resultant from executing move d in state s. Interpret this as a mapping g: S x S x D -> R, and this is clearly an extremely high-dimensional task. 

Note on notation: g(s_ | s, d) written as g(s) where board state and decision (s, d) are implicit. 


Observation: Per definition, f can be decomposed as Expectation~g(s)[p(g(s)) + f(g(s), d)] 


Problem: 

Board state values f(s, d) evidently depend on the policy a* they are based on. Clearly, we cannot find a(s) without information about either f or a*. 


Idea: Why not use the behavior of the provided simple agents as initial estimates for board state values? I.e. define a* as "simple agent". 


Pros: Learn basic behaviors 

Cons: Learned behaviors are almost surely suboptimal. However: They can perhaps be corrected through more sophisticated training later on. 



Procedure: 

Note: Due to dimensionality considerations, it may be sensible to reduce the number of features agents are exposed to. Proposal: Restrict vision of the board to a 13*13 square centered on the agent, with additional hand-designed features added. Fields outside the map would be rendered as walls. Motivation-> Maintain useful information, reduce extraneous information to simplify training. 

Define an episode e € E as an observed game played by a group of agents ("players") A under some policy p. 

Define the stripped state strip: A x S -> S'; (a, s) -> strip(a, s) ("observation") as "game state s as seen by player a € A, without knowledge of the decision made by player a during s". 



Each episode terminates after a finite number of steps Tau (assumed to be constant for all episodes). 

The step number of a game state s is given by step(s). The number of points ("score") of a player A up to and including step t is given by score(A, t).


The decision a player made in state s is given by decision(a, s). 


Define f_train: A x S -> R; (a, s) --> (score(a, Tau) - score(a, step(s))) / (Tau - step(s)) for step(s) != Tau, otherwise define f_train(a, s) as 0 




Observe many episodes between simple agents and create training dataset X subset of (A x S' x D x R) where x € X is a training datum consisting of: A given player a € A, observation s' € S' = strip(a, s) and decision(a, s) for each s € S in each episode, and the target value f_train(a, s). 


Once we have approximated this mapping with a new function act: (A x S' x D) -> R (regression task), the predicted best move is given by argmax d € D act(a, s', d) for known agent (self) and stripped ("undecided") board state s'. 




Generating X: 


To generate basic training data, run many games among simple agents and record game states along with f_train evaluated for each state and agent. In addition, training data can be manipulated by rotation by 90 degrees in each direction and reflection along the coordinate axes to generate new, valid game states. 


Problem: 


This approach of generating X will let us learn the behavior of simple agents from observed games.


But how do we expand our knowledge of gameplay to consider moves the simple agent would not make?


First idea: 


Use old episodes e € X and select random points in the game as "seed" states. Initialize a new game set to seed state s$, but choose as first move for one agent a random move. Play the rest of the game as usual. Goal: Learn what happens when new moves are chosen and followed by potentially suboptimal decisions. 


Further consideration needed to learn better policies once simple agent behavior is learned!


























