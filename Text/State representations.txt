


2. 


In order to test the efficacy of various machine learning approaches for our task, it is convenient to have a database of played games to use as a basis for training. 


The alternative to this approach - playing games and training models "live", without saving them to the disk somehow - has the disadvantage that all games played are "lost" and cannot be reused, and that one cannot compare the performance of different models to each other on the same data. The author presumes it is also slower to train in this way, rather than doing all training operations on a single array in memory. 


For these reasons, the team chose to develop a vectorized representation of the game state which contains all information for a single game step. A successful state representation is space-efficient and contains all game-characterizing events. In order to save space, we assumed that games would always have 4 players, and that players could deploy at most one bomb. Player have either a bomb in their inventory or a bomb on the playing field. Other game design features, such as the size of the board or the number of events that can occur, do not affect the representability of the game state. 


In the following discussion, a standard board format of a 17 x 17 grid is assumed. The state representation takes the form of a numpy integer array and contains 3 separate parts:


**Insert graphic showing vector format**

**Insert number encoding conventions**


The first is the board state, which consists of 176 potentially navigable fields and 113 wall fields. In order to remove redundant information, the state representation only notes the contents of the navigable fields. Board state fields contain information about: Crates, Coins, Bombs, and Explosions. 


To represent the board state, each navigable field is uniquely associated with an index in range(1, 177). Indices count through all x values in one y column of the board matrix, skipping all wall fields, before advancing to the next y column. Index 1 is associated with board coordinates (1, 1), Index 2 with board coordinates (2, 1), and so on. 


Because the game provides location information on basis of x, y coordinates, it was necessary to write functions to translate between the two coordinate representations. Initially, we had implemented these functions to arithmetically translate between index and x, y coordinates when called, but run profiling revealed that these function calls were taking up the bulk of training time. We then rewrote the functions to provide indices as a table lookup


**Insert player block representation here**

The next segment consists of four "player blocks". These contain: Information about player and bomb locations, bomb timers, player score for the current game, and the events that were passed to each player by the end of a game state. Players are written into the player blocks in the order established by the environment attribute "players". 

Dead players and bombs in inventory (not on board) are noted with the board index "0". Bombs in inventory have timers "0". 


As of (18.03.2019), the game provides information about 17 distinct events. For each player, a block of length 17 keeps track of the occurence of each game event for this player in the current game step. Events are noted in the order they occur in settings.py; certain events, such as "CRATE_DESTROYED", can occur multiple times for one player in one game step, and are noted according to their multiplicity in the current game step. Events that do not occur are noted as "0".


Our initial approach was to note each event as a boolean, but this had the crucial disadvantage that, during training, one could not give multiple rewards for an event that occured multiple times, sich as when one player kills multiple enemies in a single step. 


Lastly, the current step number is noted as a single integer in the last position of the state vector. In total, the state vector contains 176 + 4*21 + 1 = 261 integers to define the game state. 




Implementation of state saving


In order to generate bulk data for efficient training later on, it proved helpful to manage game state saving from the game's environment file. To implement this, a function capture_state() was called at the end of do_step(), and the resultant array appended to a new member save_list of type "list". 


capture_state loops through all players, bombs, coins, and explosions and saves them to the vector, as well as noting the presence of crates in the map. 


At the end of the environment function end_round, all states are condensed to a numpy array and saved to the disk using a timestamped filename defined during intialization of the environment, as well as the "round number" (defined in settings.py, allowing a script to run an arbitrary number of games and save each as a distinct file with common timestamp). 


Doing it this way, rather than outsourcing game state saving to the agents themselves, reduced game state saving to a single, centralized process and avoided problems resultant from managing living vs. dead players, etc. 


The main drawback of this approach, as far as training goes, is that at agent execution time, agents do not have access to the environment (not to mention that the tournament uses an unmodified version of the environment which does not implement our version of state saving). 


To circumvent this, the team was forced to make the unelegant but necessary design choice of implementing a second, equivalent state saving function "derive_state_representation" that is accessible to agent code at runtime, which constructs the state vector from the attributes available in the dictionary game_state. It proved to be a challenge, during development, to verify that both functions behave in equivalent ways. 


Complicating this process is the fact that it is not possible to derive which bomb belongs to which player from the information in game_state. However, as we shall see in **Observations chapter**, it is for our purposes unimportant which player a bomb belongs to, as all bombs are equally lethal and what matters during combat is which player can potentially deploy a bomb. 


Here, the state vector is constructed in the same way, except that bombs are assigned to players more or less randomly, on the basis of whether they currently hold a bomb in inventory or not. In addition, the player calling the state derivation function is always indexed as player 0 in the game state, which is used later on to create this player's observation. 























