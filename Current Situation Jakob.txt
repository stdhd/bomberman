


Current situation: 
----------------


We want to design an agent (function) a: S -> D which, given an observation s' € S' of a board state s € S, chooses among actions ("decisions") D to select the action with the highest expected placement at the end of the game. Each board state is (in the case of Bomberman strictly deterministically) associated with a number of points p(s) € R. Points are collected over the course of the game, and the agent with the highest number of points at the end wins the game. Player placements are in decreasing order of number of points scored. 



Observations: 

#  High placement <=> having many points at the end. Assuming a fixed and not necessarily known policy function a*: S -> D:

#  Move d in board state s leads to more points won until the end of the game than move d' => d results in more points won by the end of the game than d' <=> d results in a greater or equal placement than d' from board state s

#  The optimal move is that move which maximizes the expected number of points won until the end of the game (AFTER the current state) <=> maximizes the expected number of points won per remaining move

#  => Define value function f(s, d) := "Expected number of points won per move until the end of the game from board state s given move d"

#  Per observation 1), argmax (d € D) f(s, d) =: a(s) is the optimal move in board state s. 


Problem: 


How do we learn how many points a board state + move is expected to result in, i.e. how can we learn about f? 

Given (s, d) € S x D , define g(s_ | s, d) as probability mass function over board states resultant from executing move d in state s. Interpret this as a mapping g: S x S x D -> R, and this is clearly an extremely high-dimensional task. 

Note on notation: g(s_ | s, d) written as g(s) where board state and decision (s, d) are implicit. 


Observation: Per definition, f can be decomposed as Expectation~g(s)[p(g(s)) + f(g(s), d)] 


Problem: 

Board state values f(s, d) evidently depend on the policy a* they are based on. Clearly, we cannot find a(s) without information about either f or a*. 


Idea: Why not use the behavior of the provided simple agents as initial estimates for board state values? I.e. define a* as "simple agent". 


Pros: Learn basic behaviors 

Cons: Learned behaviors are almost surely suboptimal. However: They can perhaps be corrected through more sophisticated training later on. 



Procedure: 

Note: Due to dimensionality considerations, it may be sensible to reduce the number of features agents are exposed to. Proposal: Restrict vision of the board to a 13*13 square centered on the agent, with additional hand-designed features added. Fields outside the map would be rendered as walls. Motivation-> Maintain useful information, reduce extraneous information to simplify training. 

Define an episode e € E as an observed game played by a group of agents ("players") A under some policy p. 

Define the stripped state strip: A x S -> S'; (a, s) -> strip(a, s) ("observation") as "game state s as seen by player a € A, without knowledge of the decision made by player a during s". 



Each episode terminates after a finite number of steps Tau (assumed to be constant for all episodes). 

The step number of a game state s is given by step(s). The number of points ("score") of a player A up to and including step t is given by score(A, t).


The decision a player made in state s is given by decision(a, s). 


Define f_train: A x S -> R; (a, s) --> (score(a, Tau) - score(a, step(s))) / (Tau - step(s)) for step(s) != Tau, otherwise define f_train(a, s) as 0 




Observe many episodes between simple agents and create training dataset X subset of (A x S' x D x R) where x € X is a training datum consisting of: A given player a € A, observation s' € S' = strip(a, s) and decision(a, s) for each s € S in each episode, and the target value f_train(a, s). 


Once we have approximated this mapping with a new function act: (A x S' x D) -> R (regression task), the predicted best move is given by argmax d € D act(a, s', d) for known agent (self) and observed board state s'. 




Generating X: 


To generate basic training data, run many games among simple agents and record game states along with f_train evaluated for each state and agent. In addition, training data can be manipulated by rotation by 90 degrees in each direction and reflection along the coordinate axes to generate new, valid game states. 


Problem: 


This approach of generating X will let us learn the behavior of simple agents from observed games.


But how do we expand our knowledge of gameplay to consider moves the simple agent would not make?


First idea: 


Use old episodes e € X and select random points in the game as "seed" states. Initialize a new game set to seed state s$, but choose as first move for one agent a random move. Play the rest of the game as usual. Goal: Learn what happens when new moves are chosen and followed by known optimal decisions. 


Further consideration needed to learn better policies once simple agent behavior is learned!


















